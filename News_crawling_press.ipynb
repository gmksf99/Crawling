{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀레니움은 원하는 언론사 선택\n",
    "# 아름다운비누로 크롤링"
    "- 셀레니움 부분  : https://everyday-tech.tistory.com/entry/3%ED%83%84-%EC%89%BD%EA%B2%8C-%EB%94%B0%EB%9D%BC%ED%95%98%EB%8A%94-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%89%B4%EC%8A%A4-%ED%81%AC%EB%A1%A4%EB%A7%81-%EB%B3%B8%EB%AC%B8-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import pickle, json, glob, time\n",
    "from tqdm import tqdm\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 키워드  : 코로나 확진자 발생\n",
      "수집 뉴스의 수(숫자만 입력) : 150\n",
      "시작 날짜를 입력해주세요(ex.2021.01.01) : 2021.01.01\n",
      "끝 날짜를 입력해주세요(ex.2021.01.31) : 2021.01.10\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "\n",
      "---크롤링 시작---\n",
      "0 번째 뉴스를 추출을 완료했습니다\n",
      "1 번째 뉴스를 추출을 완료했습니다\n",
      "2 번째 뉴스를 추출을 완료했습니다\n",
      "3 번째 뉴스를 추출을 완료했습니다\n",
      "4 번째 뉴스를 추출을 완료했습니다\n",
      "5 번째 뉴스를 추출을 완료했습니다\n",
      "6 번째 뉴스를 추출을 완료했습니다\n",
      "7 번째 뉴스를 추출을 완료했습니다\n",
      "8 번째 뉴스를 추출을 완료했습니다\n",
      "9 번째 뉴스를 추출을 완료했습니다\n",
      "10 번째 뉴스를 추출을 완료했습니다\n",
      "11 번째 뉴스를 추출을 완료했습니다\n",
      "12 번째 뉴스를 추출을 완료했습니다\n",
      "13 번째 뉴스를 추출을 완료했습니다\n",
      "14 번째 뉴스를 추출을 완료했습니다\n",
      "15 번째 뉴스를 추출을 완료했습니다\n",
      "16 번째 뉴스를 추출을 완료했습니다\n",
      "17 번째 뉴스를 추출을 완료했습니다\n",
      "18 번째 뉴스를 추출을 완료했습니다\n",
      "19 번째 뉴스를 추출을 완료했습니다\n",
      "20 번째 뉴스를 추출을 완료했습니다\n",
      "21 번째 뉴스를 추출을 완료했습니다\n",
      "22 번째 뉴스를 추출을 완료했습니다\n",
      "23 번째 뉴스를 추출을 완료했습니다\n",
      "24 번째 뉴스를 추출을 완료했습니다\n",
      "25 번째 뉴스를 추출을 완료했습니다\n",
      "26 번째 뉴스를 추출을 완료했습니다\n",
      "27 번째 뉴스를 추출을 완료했습니다\n",
      "28 번째 뉴스를 추출을 완료했습니다\n",
      "29 번째 뉴스를 추출을 완료했습니다\n",
      "30 번째 뉴스를 추출을 완료했습니다\n",
      "31 번째 뉴스를 추출을 완료했습니다\n",
      "32 번째 뉴스를 추출을 완료했습니다\n",
      "33 번째 뉴스를 추출을 완료했습니다\n",
      "34 번째 뉴스를 추출을 완료했습니다\n",
      "35 번째 뉴스를 추출을 완료했습니다\n",
      "36 번째 뉴스를 추출을 완료했습니다\n",
      "37 번째 뉴스를 추출을 완료했습니다\n",
      "38 번째 뉴스를 추출을 완료했습니다\n",
      "39 번째 뉴스를 추출을 완료했습니다\n",
      "40 번째 뉴스를 추출을 완료했습니다\n",
      "41 번째 뉴스를 추출을 완료했습니다\n",
      "42 번째 뉴스를 추출을 완료했습니다\n",
      "43 번째 뉴스를 추출을 완료했습니다\n",
      "44 번째 뉴스를 추출을 완료했습니다\n",
      "45 번째 뉴스를 추출을 완료했습니다\n",
      "46 번째 뉴스를 추출을 완료했습니다\n",
      "47 번째 뉴스를 추출을 완료했습니다\n",
      "48 번째 뉴스를 추출을 완료했습니다\n",
      "49 번째 뉴스를 추출을 완료했습니다\n",
      "50 번째 뉴스를 추출을 완료했습니다\n",
      "51 번째 뉴스를 추출을 완료했습니다\n",
      "52 번째 뉴스를 추출을 완료했습니다\n",
      "53 번째 뉴스를 추출을 완료했습니다\n",
      "54 번째 뉴스를 추출을 완료했습니다\n",
      "55 번째 뉴스를 추출을 완료했습니다\n",
      "56 번째 뉴스를 추출을 완료했습니다\n",
      "57 번째 뉴스를 추출을 완료했습니다\n",
      "58 번째 뉴스를 추출을 완료했습니다\n",
      "59 번째 뉴스를 추출을 완료했습니다\n",
      "60 번째 뉴스를 추출을 완료했습니다\n",
      "61 번째 뉴스를 추출을 완료했습니다\n",
      "62 번째 뉴스를 추출을 완료했습니다\n",
      "63 번째 뉴스를 추출을 완료했습니다\n",
      "64 번째 뉴스를 추출을 완료했습니다\n",
      "65 번째 뉴스를 추출을 완료했습니다\n",
      "66 번째 뉴스를 추출을 완료했습니다\n",
      "67 번째 뉴스를 추출을 완료했습니다\n",
      "68 번째 뉴스를 추출을 완료했습니다\n",
      "69 번째 뉴스를 추출을 완료했습니다\n",
      "70 번째 뉴스를 추출을 완료했습니다\n",
      "71 번째 뉴스를 추출을 완료했습니다\n",
      "72 번째 뉴스를 추출을 완료했습니다\n",
      "73 번째 뉴스를 추출을 완료했습니다\n",
      "74 번째 뉴스를 추출을 완료했습니다\n",
      "75 번째 뉴스를 추출을 완료했습니다\n",
      "76 번째 뉴스를 추출을 완료했습니다\n",
      "77 번째 뉴스를 추출을 완료했습니다\n",
      "78 번째 뉴스를 추출을 완료했습니다\n",
      "79 번째 뉴스를 추출을 완료했습니다\n",
      "80 번째 뉴스를 추출을 완료했습니다\n",
      "81 번째 뉴스를 추출을 완료했습니다\n",
      "82 번째 뉴스를 추출을 완료했습니다\n",
      "83 번째 뉴스를 추출을 완료했습니다\n",
      "84 번째 뉴스를 추출을 완료했습니다\n",
      "85 번째 뉴스를 추출을 완료했습니다\n",
      "86 번째 뉴스를 추출을 완료했습니다\n",
      "87 번째 뉴스를 추출을 완료했습니다\n",
      "88 번째 뉴스를 추출을 완료했습니다\n",
      "89 번째 뉴스를 추출을 완료했습니다\n",
      "90 번째 뉴스를 추출을 완료했습니다\n",
      "91 번째 뉴스를 추출을 완료했습니다\n",
      "92 번째 뉴스를 추출을 완료했습니다\n",
      "93 번째 뉴스를 추출을 완료했습니다\n",
      "94 번째 뉴스를 추출을 완료했습니다\n",
      "95 번째 뉴스를 추출을 완료했습니다\n",
      "96 번째 뉴스를 추출을 완료했습니다\n",
      "97 번째 뉴스를 추출을 완료했습니다\n",
      "98 번째 뉴스를 추출을 완료했습니다\n",
      "99 번째 뉴스를 추출을 완료했습니다\n",
      "100 번째 뉴스를 추출을 완료했습니다\n",
      "101 번째 뉴스를 추출을 완료했습니다\n",
      "102 번째 뉴스를 추출을 완료했습니다\n",
      "103 번째 뉴스를 추출을 완료했습니다\n",
      "104 번째 뉴스를 추출을 완료했습니다\n",
      "105 번째 뉴스를 추출을 완료했습니다\n",
      "106 번째 뉴스를 추출을 완료했습니다\n",
      "107 번째 뉴스를 추출을 완료했습니다\n",
      "108 번째 뉴스를 추출을 완료했습니다\n",
      "109 번째 뉴스를 추출을 완료했습니다\n",
      "110 번째 뉴스를 추출을 완료했습니다\n",
      "111 번째 뉴스를 추출을 완료했습니다\n",
      "112 번째 뉴스를 추출을 완료했습니다\n",
      "113 번째 뉴스를 추출을 완료했습니다\n",
      "114 번째 뉴스를 추출을 완료했습니다\n",
      "115 번째 뉴스를 추출을 완료했습니다\n",
      "116 번째 뉴스를 추출을 완료했습니다\n",
      "117 번째 뉴스를 추출을 완료했습니다\n",
      "118 번째 뉴스를 추출을 완료했습니다\n",
      "119 번째 뉴스를 추출을 완료했습니다\n",
      "120 번째 뉴스를 추출을 완료했습니다\n",
      "121 번째 뉴스를 추출을 완료했습니다\n",
      "122 번째 뉴스를 추출을 완료했습니다\n",
      "123 번째 뉴스를 추출을 완료했습니다\n",
      "124 번째 뉴스를 추출을 완료했습니다\n",
      "125 번째 뉴스를 추출을 완료했습니다\n",
      "126 번째 뉴스를 추출을 완료했습니다\n",
      "127 번째 뉴스를 추출을 완료했습니다\n",
      "128 번째 뉴스를 추출을 완료했습니다\n",
      "129 번째 뉴스를 추출을 완료했습니다\n",
      "130 번째 뉴스를 추출을 완료했습니다\n",
      "131 번째 뉴스를 추출을 완료했습니다\n",
      "132 번째 뉴스를 추출을 완료했습니다\n",
      "133 번째 뉴스를 추출을 완료했습니다\n",
      "134 번째 뉴스를 추출을 완료했습니다\n",
      "135 번째 뉴스를 추출을 완료했습니다\n",
      "136 번째 뉴스를 추출을 완료했습니다\n",
      "137 번째 뉴스를 추출을 완료했습니다\n",
      "138 번째 뉴스를 추출을 완료했습니다\n",
      "139 번째 뉴스를 추출을 완료했습니다\n",
      "140 번째 뉴스를 추출을 완료했습니다\n",
      "141 번째 뉴스를 추출을 완료했습니다\n",
      "142 번째 뉴스를 추출을 완료했습니다\n",
      "143 번째 뉴스를 추출을 완료했습니다\n",
      "144 번째 뉴스를 추출을 완료했습니다\n",
      "145 번째 뉴스를 추출을 완료했습니다\n",
      "146 번째 뉴스를 추출을 완료했습니다\n",
      "147 번째 뉴스를 추출을 완료했습니다\n",
      "148 번째 뉴스를 추출을 완료했습니다\n",
      "149 번째 뉴스를 추출을 완료했습니다\n",
      "---크롤링 종료---\n"
     ]
    }
   ],
   "source": [
    "sleep_sec = 0.5\n",
    "\n",
    "press_list = ['경향신문', '국민일보', '내일신문', '동아일보', '매일일보', '문화일보', '서울신문', \n",
    "              '전국매일신문', '조선일보', '중앙일보', '천지일보', '한겨레', '한국일보']\n",
    "\n",
    "############### 브라우저를 켜고 검색 키워드 입력 ####################\n",
    "query = input('검색할 키워드  : ')\n",
    "news_num = int(input('수집 뉴스의 수(숫자만 입력) : '))\n",
    "ds = input('시작 날짜를 입력해주세요(ex.2021.01.01) : ') \n",
    "de = input('끝 날짜를 입력해주세요(ex.2021.01.31) : ')\n",
    "\n",
    "print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "chrome_path = 'C:/Users/이수빈/Machine learning/chromedriver.exe'\n",
    "browser = webdriver.Chrome(chrome_path)\n",
    "\n",
    "news_url = 'https://search.naver.com/search.naver?where=news&query={}&sm=tab_srt&sort=2&photo=0&field=1&reporter_article=&pd=3&ds={}&de={}'.format(query, ds, de)\n",
    "browser.get(news_url)\n",
    "time.sleep(sleep_sec)\n",
    "\n",
    "\n",
    "######### 언론사 선택 및 confirm #####################\n",
    "print('설정한 언론사를 선택합니다.\\n')\n",
    "search_opt_box = browser.find_element_by_xpath('//*[@id=\"search_option_button\"]')\n",
    "search_opt_box.click()\n",
    "time.sleep(0.02)\n",
    "\n",
    "# 언론사 선택하는 바를 활성화\n",
    "tablist_box = browser.find_element_by_xpath('//div[@class=\"snb_inner\"]/ul[@role=\"tablist\" and @class=\"option_menu\"]')\n",
    "\n",
    "tablist_elem_list = tablist_box.find_elements_by_xpath('./li[@role=\"presentation\"]')\n",
    "press_box = [t for t in tablist_elem_list if t.text == '언론사'][0].find_element_by_xpath('./a')\n",
    "press_box.click()\n",
    "\n",
    "# 언론사 종류 하나씩 선택\n",
    "actived_press_frame = browser.find_element_by_xpath('.//div[@class=\"snb_itembox lst_press _search_option_press_\"]')\n",
    "total_press_box = actived_press_frame.find_element_by_xpath('./div[@class=\"group_sort type_press _group_by_press_\"]')\n",
    "\n",
    "# 언론사 종류를 선택하는 버튼이 담긴 박스\n",
    "press_cat_active_button = total_press_box.find_elements_by_xpath('.//a[@role=\"tab\" and @class=\"item _tab_filter_\"]') # 언론사 종류 하나씩 버튼\n",
    "press_cat_active_button_dict = dict(zip([t.text for t in press_cat_active_button], press_cat_active_button)) # 언론사 종류 이름 : 언론사 종류 활성화 버튼\n",
    "\n",
    "# 밑에 각 언론사 종류별 개별 언론사가 담겨있는 박스들\n",
    "each_press_box_list = total_press_box.find_elements_by_xpath('.//div[@class=\"scroll_area _panel_filter_\"]')\n",
    "\n",
    "# 1. 언론사 종류 1개 선택\n",
    "# 2. 선택한 언론사 종류에 해당하는 개별 언론사 중 크롤링할 언론사에 포함되는 것 체크 \n",
    "for idx, press_cat_name in enumerate(press_cat_active_button_dict.keys()):\n",
    "    #하나의 언론사 종류를 클릭해서 활성화시킴\n",
    "    press_cat_active_button_dict[press_cat_name].click()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "    # 선택한 언론사 종류 안의 개별 언론사가 담긴 박스\n",
    "    each_press_box = each_press_box_list[idx].find_element_by_xpath('./div[@class=\"select_item\"]')\n",
    "    # 개별 언론사의 이름\n",
    "    each_press_title_list = [ep.get_attribute('title') for ep in each_press_box.find_elements_by_xpath('.//label')]\n",
    "    # 개별 언론사 체크 박스\n",
    "    each_press_input_list = each_press_box.find_elements_by_xpath('.//input')\n",
    "    \n",
    "    # 딕셔너리(개별 언론사 이름 : 개별 언론사 체크 박스)\n",
    "    each_press_title_input_dict = dict(zip(each_press_title_list, each_press_input_list))\n",
    "    # 추출하고 싶은 언론사 존재 시 체크박스 클릭\n",
    "    for title in [tit for tit in each_press_title_input_dict.keys() if tit in press_list]:\n",
    "        #print(title)\n",
    "        each_press_title_input_dict[title].click()\n",
    "\n",
    "\n",
    "# 확인 버튼\n",
    "confirm_buttons = actived_press_frame.find_element_by_xpath('./span[@class=\"btn_inp\"]').find_elements_by_xpath('.//button')\n",
    "ok_button = [c for c in confirm_buttons if c.text == '확인'][0]\n",
    "ok_button.click()\n",
    "\n",
    "################## 크롤링 #####################\n",
    "# 뉴스 날짜, 언론사, 제목 크롤링\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'\n",
    "\n",
    "req = browser.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')\n",
    "\n",
    "news_dict = {}\n",
    "idx = 0\n",
    "cur_page = 1\n",
    "\n",
    "print()\n",
    "print('---크롤링 시작---')\n",
    "\n",
    "while idx < news_num:\n",
    "    table = soup.find('ul',{'class' : 'list_news'})\n",
    "    li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "    area_list = [li.find('div', {'class' : 'news_area'}) for li in li_list]\n",
    "    a_list = [area.find('a', {'class' : 'news_tit'}) for area in area_list]\n",
    "    span_list = [area.find('span', {'class' : 'info'}) for area in area_list]\n",
    "    press = [area.find('a', {'class' : 'info press'}) for area in area_list]\n",
    "    i = 0\n",
    "    i_n = len(span_list)\n",
    "    word = '언론사 선정' # 언론사 이름 뒤에 같이 나오는 경우가 있어서 삭제 해줌\n",
    "            \n",
    "    for n in a_list[:min(len(a_list), news_num-idx)]:\n",
    "        p = press[i].get_text()\n",
    "        if i < i_n:\n",
    "            if word in p:\n",
    "                p = p[:-6]\n",
    "            \n",
    "            news_dict[idx] = {'date' : span_list[i].get_text(), 'press' : p , 'title' : n.get('title')} \n",
    "            print(idx, '번째 뉴스를 추출을 완료했습니다')\n",
    "            \n",
    "        idx += 1\n",
    "        i += 1\n",
    "    cur_page += 1\n",
    "\n",
    "    pages = soup.find('div', {'class' : 'sc_page_inner'})\n",
    "    \n",
    "    next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
    "    \n",
    "    req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "print('---크롤링 종료---')\n",
    "\n",
    "news_df = DataFrame(news_dict).T\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "csv_name = '네이버뉴스_{}_{}.csv'.format(query, date)\n",
    "\n",
    "news_df.to_csv(csv_name, encoding = 'utf-8-sig')\n",
    "\n",
    "# print('엑셀 저장 완료 | 경로 : {}\\\\{}'.format(folder_path, xlsx_file_name))\n",
    "# os.startfile(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?&where=news&query=%EC%BD%94%EB%A1%9C%EB%82%98%20%ED%99%95%EC%A7%84%EC%9E%90%20%EB%B0%9C%EC%83%9D&sm=tab_pge&sort=2&photo=0&field=1&reporter_article=&pd=3&ds=2021.01.01&de=2021.01.10&docid=&nso=so:da,p:from20210101to20210110,a:t&mynews=1&start=151'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
    "next_page_url\n",
    "req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "'https://search.naver.com/search.naver' + next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>press</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021.01.01.</td>\n",
       "      <td>천지일보</td>\n",
       "      <td>[광주 코로나 현황] 밤사이 신규 확진자 10명(광주 #1089~1098)추가 발생</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021.01.01.</td>\n",
       "      <td>천지일보</td>\n",
       "      <td>[군산 코로나 현황] 신규 확진자 4명 발생(군산 120~123번)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021.01.01.</td>\n",
       "      <td>천지일보</td>\n",
       "      <td>[전남 코로나 현황] 순천 가족감염 확산 등 확진자 8명 추가 발생</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>천지일보</td>\n",
       "      <td>[광주 코로나 현황] 어제오늘 지역감염 확진자 20명 발생 … '최근 일주일간 가족...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>천지일보</td>\n",
       "      <td>[순천 코로나 현황] 확진자 5명 추가 발생… 감염경로 조사 중</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>강원도민일보</td>\n",
       "      <td>춘천 코로나19 확진자 1명(165번) 추가 발생</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>이투데이</td>\n",
       "      <td>[코로나19 현황] 국내 확진자 '총 6만2593명' 824명 추가…지역발생 확진자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>부산일보</td>\n",
       "      <td>경남에서 코로나19 확진자 14명 발생…도내 누적 1371명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>경북신문</td>\n",
       "      <td>[전문] 대구 `코로나19` 확진자 29명 발생...교회·의료기관 집단감염 `연이어`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2021.01.02.</td>\n",
       "      <td>충청리뷰</td>\n",
       "      <td>원주시청 '종교시설 관련 포함 코로나 확진자 10명 추가 발생'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   press                                              title\n",
       "0    2021.01.01.    천지일보     [광주 코로나 현황] 밤사이 신규 확진자 10명(광주 #1089~1098)추가 발생\n",
       "1    2021.01.01.    천지일보              [군산 코로나 현황] 신규 확진자 4명 발생(군산 120~123번)\n",
       "2    2021.01.01.    천지일보              [전남 코로나 현황] 순천 가족감염 확산 등 확진자 8명 추가 발생\n",
       "3    2021.01.02.    천지일보  [광주 코로나 현황] 어제오늘 지역감염 확진자 20명 발생 … '최근 일주일간 가족...\n",
       "4    2021.01.02.    천지일보                [순천 코로나 현황] 확진자 5명 추가 발생… 감염경로 조사 중\n",
       "..           ...     ...                                                ...\n",
       "145  2021.01.02.  강원도민일보                        춘천 코로나19 확진자 1명(165번) 추가 발생\n",
       "146  2021.01.02.    이투데이  [코로나19 현황] 국내 확진자 '총 6만2593명' 824명 추가…지역발생 확진자...\n",
       "147  2021.01.02.    부산일보                  경남에서 코로나19 확진자 14명 발생…도내 누적 1371명\n",
       "148  2021.01.02.    경북신문    [전문] 대구 `코로나19` 확진자 29명 발생...교회·의료기관 집단감염 `연이어`\n",
       "149  2021.01.02.    충청리뷰                원주시청 '종교시설 관련 포함 코로나 확진자 10명 추가 발생'\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "croawing = pd.read_csv(csv_name)\n",
    "croawing = croawing.drop(columns = ['Unnamed: 0'])\n",
    "croawing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
